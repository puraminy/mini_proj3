{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAN-CFAR.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/puraminy/mini_proj3/blob/master/GAN_CFAR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "n_0j_uVUcqiG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "af68c2e7-f7ce-4fac-b7a7-519e6226c3ba"
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import tensorflow.contrib.layers as lays\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import tarfile\n",
        "import urllib\n",
        "\n",
        "from keras.datasets import cifar10\n",
        "\n",
        "lr = 0.0002\n",
        "num_epochs = 200\n",
        "batch_size = 50\n",
        "# data_path = 'data/cifar-10-batches-py/{}'\n",
        "\n",
        "# # Download weights\n",
        "# if not os.path.isdir('data'):\n",
        "#     os.makedirs('data')\n",
        "# if not os.path.isfile('data/cifar-10-batches-py/data_batch_1'):\n",
        "#     print('Downloading the data ...')\n",
        "#     urllib.urlretrieve(\"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\", \"data/cifar-10-python.tar.gz\")\n",
        "#     with tarfile.open('data/cifar-10-python.tar.gz', \"r:gz\") as tar:\n",
        "#         tar.extractall('data/')\n",
        "#     os.remove('data/cifar-10-python.tar.gz')\n",
        "#     print('Download is complete !')\n",
        "\n",
        "# # unpickle data files\n",
        "# def unpickle(file):\n",
        "#     import cPickle\n",
        "#     with open(file, 'rb') as fo:\n",
        "#         dict = cPickle.load(fo)\n",
        "#     return dict\n",
        "\n",
        "# # Load all the data files and stack them together\n",
        "# dataset_x = unpickle(data_path.format('data_batch_1'))['data']\n",
        "# for i in range(2,6):\n",
        "#     dataset_x = np.vstack((dataset_x, unpickle(data_path.format('data_batch_{}'.format(i)))['data']))\n",
        "    \n",
        "(dataset_x, y),(_,_) = cifar10.load_data() \n",
        "# calculate the number of batches per epoch\n",
        "batch_per_ep = dataset_x.shape[0]//batch_size\n",
        "\n",
        "\n",
        "def next_batch(batch_number):\n",
        "    # a function to load a bacth of images\n",
        "    batch_x = dataset_x[(batch_number) * batch_size:min([((batch_number) + 1) * batch_size, dataset_x.shape[0]]), :]\n",
        "\n",
        "    # reshape the sample to a batch of images\n",
        "    batch_img = batch_x.reshape((-1, 3, 32, 32))/255.0\n",
        "    batch_img = batch_img.transpose([0, 2, 3, 1])\n",
        "    return batch_img\n",
        "\n",
        "\n",
        "def leaky_relu(x, alpha=0.1):\n",
        "    # Leaky Relu activation function\n",
        "    m_x = tf.nn.relu(-x)\n",
        "    x = tf.nn.relu(x)\n",
        "    x -= alpha * m_x\n",
        "    return x\n",
        "\n",
        "# Define the Generator, a simple CNN with 1 fully connected and 4 convolution layers\n",
        "def generator(inputs, reuse=False):\n",
        "    with tf.variable_scope('generator'):\n",
        "        if reuse:\n",
        "            tf.get_variable_scope().reuse_variables()\n",
        "        net = lays.fully_connected(inputs, 4*4*256, scope='fc1')\n",
        "        net = tf.reshape(net, (batch_size, 4, 4, 256))\n",
        "        net = lays.conv2d_transpose(net, 128, 3, stride=2, scope='conv1', padding='SAME', activation_fn=leaky_relu)\n",
        "        net = lays.conv2d_transpose(net, 64, 3, stride=2, scope='conv2', padding='SAME', activation_fn=leaky_relu)\n",
        "        net = lays.conv2d_transpose(net, 64, 3, stride=2, scope='conv3', padding='SAME', activation_fn=leaky_relu)\n",
        "        net = lays.conv2d(net, 3, 3, scope='conv4', padding='SAME', activation_fn=tf.nn.tanh)\n",
        "        return net\n",
        "\n",
        "\n",
        "# Define the Discriminator, a simple CNN with 3 convolution and 2 fully connected layers\n",
        "def discriminator(inputs, reuse=False):\n",
        "    with tf.variable_scope('discriminator'):\n",
        "        if reuse:\n",
        "            tf.get_variable_scope().reuse_variables()\n",
        "        net = lays.conv2d_transpose(inputs, 64, 3, stride=1, scope='conv1', padding='SAME', activation_fn=leaky_relu)\n",
        "        net = lays.max_pool2d(net, 2, 2, 'SAME', scope='max1')\n",
        "        net = lays.conv2d_transpose(net, 128, 3, stride=1, scope='conv2', padding='SAME', activation_fn=leaky_relu)\n",
        "        net = lays.max_pool2d(net, 2, 2, 'SAME', scope='max2')\n",
        "        net = lays.conv2d_transpose(net, 256, 3, stride=1, scope='conv3', padding='SAME', activation_fn=leaky_relu)\n",
        "        net = lays.max_pool2d(net, 2, 2, 'SAME', scope='max3')\n",
        "        net = tf.reshape(net, (batch_size, 4 * 4 * 256))\n",
        "        net = lays.fully_connected(net, 128, scope='fc1', activation_fn=leaky_relu)\n",
        "        net = lays.dropout(net, 0.5)\n",
        "        net = lays.fully_connected(net, 1, scope='fc2', activation_fn=None)\n",
        "        return net\n",
        "\n",
        "\n",
        "images = tf.placeholder(tf.float32, (batch_size, 32, 32, 3))    # input images\n",
        "z_in = tf.placeholder(tf.float32, (batch_size, 100))            # input noises\n",
        "\n",
        "# Train the discriminator, it tries to discriminate between real and fake (generated) samples\n",
        "outputs_real = discriminator(images)\n",
        "loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(outputs_real), logits=outputs_real))\n",
        "\n",
        "images_fake = generator(z_in)\n",
        "outputs_fake = discriminator(images_fake, reuse=True)\n",
        "loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.zeros_like(outputs_fake), logits=outputs_fake))\n",
        "\n",
        "loss_d = loss_real + loss_fake  # Calculate the total loss\n",
        "discrim_tvars = [var for var in tf.trainable_variables() if var.name.startswith(\"discriminator\")]\n",
        "discrim_train = tf.train.AdamOptimizer(lr).minimize(loss_d, var_list=discrim_tvars)\n",
        "\n",
        "# Train the generator, it tries to fool the discriminator\n",
        "with tf.control_dependencies([discrim_train]):\n",
        "    outputs = discriminator(images_fake, reuse=True)\n",
        "    loss_g = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(outputs), logits=outputs))\n",
        "    gen_tvars = [var for var in tf.trainable_variables() if var.name.startswith(\"generator\")]\n",
        "    gen_train = tf.train.AdamOptimizer(lr).minimize(loss_g, var_list=gen_tvars)\n",
        "\n",
        "# Draw samples from the input distribution as a fixed test set\n",
        "# Can follow how the generator output evolves\n",
        "test_z = np.random.normal(size=(batch_size, 100))\n",
        "\n",
        "# initialize the network\n",
        "init = tf.global_variables_initializer()\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for ep in range(num_epochs):\n",
        "        for batch_n in range(batch_per_ep):  # batches loop\n",
        "            batch_img = next_batch(batch_n)\n",
        "            train_z = np.random.normal(size=(batch_size, 100))\n",
        "\n",
        "            _, gl, dl = sess.run([gen_train, loss_g, loss_d], feed_dict={images: batch_img, z_in: train_z})\n",
        "\n",
        "            if not batch_n%10:\n",
        "                print('epoch: {} - loss_d: {} - loss_g: {}'.format(ep, dl, gl))\n",
        "\n",
        "        # Save the test results after each Epoch\n",
        "        print('Testing ...')\n",
        "        images_test = sess.run([images_fake], feed_dict={z_in: test_z})[0]\n",
        "\n",
        "        if not os.path.isdir('results'):\n",
        "            os.makedirs('results')\n",
        "        for i in range(9):\n",
        "            plt.subplot(3, 3, i+1)\n",
        "            plt.imshow(images_test[i])\n",
        "            plt.savefig('results/cfar10-gan-e{}.png'.format(ep))\n",
        "            plt.show()\n",
        "        print('A new test image saved !')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 20s 0us/step\n",
            "epoch: 0 - loss_d: 1.383413314819336 - loss_g: 0.6868053674697876\n",
            "epoch: 0 - loss_d: 1.0503698587417603 - loss_g: 0.5072242021560669\n",
            "epoch: 0 - loss_d: 0.5996611714363098 - loss_g: 1.0876106023788452\n",
            "epoch: 0 - loss_d: 0.925590455532074 - loss_g: 0.7614904046058655\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}