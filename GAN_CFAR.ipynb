{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAN-CFAR.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/puraminy/mini_proj3/blob/master/GAN_CFAR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "n_0j_uVUcqiG",
        "colab_type": "code",
        "outputId": "9bf221aa-eb50-44e7-e1e1-11170d9108e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1018
        }
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import tensorflow.contrib.layers as lays\n",
        "import tensorflow.layers as layers\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import tarfile\n",
        "import urllib\n",
        "\n",
        "from keras.datasets import cifar10\n",
        "\n",
        "lr = 0.0002\n",
        "num_epochs = 20\n",
        "batch_size = 512\n",
        "# data_path = 'data/cifar-10-batches-py/{}'\n",
        "\n",
        "# # Download weights\n",
        "# if not os.path.isdir('data'):\n",
        "#     os.makedirs('data')\n",
        "# if not os.path.isfile('data/cifar-10-batches-py/data_batch_1'):\n",
        "#     print('Downloading the data ...')\n",
        "#     urllib.urlretrieve(\"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\", \"data/cifar-10-python.tar.gz\")\n",
        "#     with tarfile.open('data/cifar-10-python.tar.gz', \"r:gz\") as tar:\n",
        "#         tar.extractall('data/')\n",
        "#     os.remove('data/cifar-10-python.tar.gz')\n",
        "#     print('Download is complete !')\n",
        "\n",
        "# # unpickle data files\n",
        "# def unpickle(file):\n",
        "#     import cPickle\n",
        "#     with open(file, 'rb') as fo:\n",
        "#         dict = cPickle.load(fo)\n",
        "#     return dict\n",
        "\n",
        "# # Load all the data files and stack them together\n",
        "# dataset_x = unpickle(data_path.format('data_batch_1'))['data']\n",
        "# for i in range(2,6):\n",
        "#     dataset_x = np.vstack((dataset_x, unpickle(data_path.format('data_batch_{}'.format(i)))['data']))\n",
        "    \n",
        "(dataset_x, y),(_,_) = cifar10.load_data() \n",
        "# calculate the number of batches per epoch\n",
        "batch_per_ep = dataset_x.shape[0]//batch_size\n",
        "\n",
        "\n",
        "def next_batch(batch_number):\n",
        "    # a function to load a bacth of images\n",
        "    batch_x = dataset_x[(batch_number) * batch_size:min([((batch_number) + 1) * batch_size, dataset_x.shape[0]]), :]\n",
        "\n",
        "    # reshape the sample to a batch of images\n",
        "    batch_img = batch_x.reshape((-1, 3, 32, 32))/255.0\n",
        "    batch_img = batch_img.transpose([0, 2, 3, 1])\n",
        "    return batch_img\n",
        "\n",
        "\n",
        "def leaky_relu(x, alpha=0.2):\n",
        "    # Leaky Relu activation function\n",
        "    m_x = tf.nn.relu(-x)\n",
        "    x = tf.nn.relu(x)\n",
        "    x -= alpha * m_x\n",
        "    return x\n",
        "\n",
        "# Define the Generator, a simple CNN with 1 fully connected and 4 convolution layers\n",
        "def generator(inputs, reuse=False):\n",
        "    with tf.variable_scope('generator'):\n",
        "        if reuse:\n",
        "            tf.get_variable_scope().reuse_variables()\n",
        "        net = lays.fully_connected(inputs, 4*4*256, scope='fc1')\n",
        "        #net = layers.batch_normalization(net)\n",
        "        net = tf.reshape(net, (batch_size, 4, 4, 256))\n",
        "        net = lays.conv2d_transpose(net, 128, 3, stride=2, scope='conv1', padding='SAME', activation_fn=leaky_relu)\n",
        "        net = layers.batch_normalization(net)\n",
        "        net = lays.conv2d_transpose(net, 64, 3, stride=2, scope='conv2', padding='SAME', activation_fn=leaky_relu)\n",
        "        net = lays.conv2d_transpose(net, 64, 3, stride=2, scope='conv3', padding='SAME', activation_fn=leaky_relu)\n",
        "        net = lays.conv2d(net, 3, 3, scope='conv4', padding='SAME', activation_fn=tf.nn.tanh)\n",
        "        return net\n",
        "\n",
        "\n",
        "# Define the Discriminator, a simple CNN with 3 convolution and 2 fully connected layers\n",
        "def discriminator(inputs, reuse=False):\n",
        "    with tf.variable_scope('discriminator'):\n",
        "        if reuse:\n",
        "            tf.get_variable_scope().reuse_variables()\n",
        "        net = lays.conv2d_transpose(inputs, 64, 3, stride=1, scope='conv1', padding='SAME', activation_fn=leaky_relu)\n",
        "        net = lays.max_pool2d(net, 2, 2, 'SAME', scope='max1')\n",
        "        net = lays.conv2d_transpose(net, 128, 3, stride=1, scope='conv2', padding='SAME', activation_fn=leaky_relu)\n",
        "        net = lays.max_pool2d(net, 2, 2, 'SAME', scope='max2')\n",
        "        net = lays.conv2d_transpose(net, 256, 3, stride=1, scope='conv3', padding='SAME', activation_fn=leaky_relu)\n",
        "        net = lays.max_pool2d(net, 2, 2, 'SAME', scope='max3')\n",
        "        net = tf.reshape(net, (batch_size, 4 * 4 * 256))\n",
        "        net = lays.fully_connected(net, 128, scope='fc1', activation_fn=leaky_relu)\n",
        "        net = lays.dropout(net, 0.5)\n",
        "        net = lays.fully_connected(net, 1, scope='fc2', activation_fn=None)\n",
        "        return net\n",
        "\n",
        "\n",
        "images = tf.placeholder(tf.float32, (batch_size, 32, 32, 3))    # input images\n",
        "z_in = tf.placeholder(tf.float32, (batch_size, 100))            # input noises\n",
        "\n",
        "# Train the discriminator, it tries to discriminate between real and fake (generated) samples\n",
        "outputs_real = discriminator(images)\n",
        "loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(outputs_real), logits=outputs_real))\n",
        "\n",
        "images_fake = generator(z_in)\n",
        "outputs_fake = discriminator(images_fake, reuse=True)\n",
        "loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.zeros_like(outputs_fake), logits=outputs_fake))\n",
        "\n",
        "loss_d = loss_real + loss_fake  # Calculate the total loss\n",
        "discrim_tvars = [var for var in tf.trainable_variables() if var.name.startswith(\"discriminator\")]\n",
        "discrim_train = tf.train.AdamOptimizer(lr).minimize(loss_d, var_list=discrim_tvars)\n",
        "\n",
        "# Train the generator, it tries to fool the discriminator\n",
        "with tf.control_dependencies([discrim_train]):\n",
        "    outputs = discriminator(images_fake, reuse=True)\n",
        "    loss_g = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(outputs), logits=outputs))\n",
        "    gen_tvars = [var for var in tf.trainable_variables() if var.name.startswith(\"generator\")]\n",
        "    gen_train = tf.train.AdamOptimizer(lr).minimize(loss_g, var_list=gen_tvars)\n",
        "\n",
        "# Draw samples from the input distribution as a fixed test set\n",
        "# Can follow how the generator output evolves\n",
        "test_z = np.random.normal(size=(batch_size, 100))\n",
        "\n",
        "# initialize the network\n",
        "init = tf.global_variables_initializer()\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for ep in range(num_epochs):\n",
        "        for batch_n in range(batch_per_ep):  # batches loop\n",
        "            batch_img = next_batch(batch_n)\n",
        "            train_z = np.random.normal(size=(batch_size, 100))\n",
        "\n",
        "            _, gl, dl = sess.run([gen_train, loss_g, loss_d], feed_dict={images: batch_img, z_in: train_z})\n",
        "\n",
        "            if not batch_n%10:\n",
        "                print('epoch: {} - loss_d: {} - loss_g: {}'.format(ep, dl, gl))\n",
        "\n",
        "        # Save the test results after each Epoch\n",
        "        print('Testing ...')\n",
        "        images_test = sess.run([images_fake], feed_dict={z_in: test_z})[0]\n",
        "\n",
        "        if not os.path.isdir('results'):\n",
        "            os.makedirs('results')\n",
        "        for i in range(9):\n",
        "            plt.subplot(3, 3, i+1)\n",
        "            plt.imshow(images_test[i])\n",
        "            plt.savefig('results/cfar10-gan-e{}.png'.format(ep))\n",
        "            plt.show()\n",
        "        print('A new test image saved !')\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 0 - loss_d: 1.3638150691986084 - loss_g: 0.6880635619163513\n",
            "epoch: 0 - loss_d: 1.096338152885437 - loss_g: 0.48760902881622314\n",
            "epoch: 0 - loss_d: 0.8524580001831055 - loss_g: 0.8190076947212219\n",
            "epoch: 0 - loss_d: 1.4061309099197388 - loss_g: 0.6647070050239563\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-3c16f0154035>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0mtrain_z\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgen_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_g\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_d\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_in\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain_z\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbatch_n\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}